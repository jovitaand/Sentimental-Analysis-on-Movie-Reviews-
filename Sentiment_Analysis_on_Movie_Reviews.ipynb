{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PRONi7PyD2BX",
        "U7f08qyu-6sJ",
        "jdThzsyk_NH2"
      ],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jovitaand/Sentimental-Analysis-on-Movie-Reviews-/blob/main/Sentiment_Analysis_on_Movie_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem Definition:\n",
        "The goal of this project is to develop a model capable of categorizing the sentiment expressed in movie reviews into five categories: very negative, negative, neutral, positive, and very positive. This task is significant as it helps in understanding customer perceptions and can be utilized for services such as recommendation systems or market analysis."
      ],
      "metadata": {
        "id": "a7ugK868C9Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('stopwords')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "import seaborn as sns\n",
        "sns.set(style='whitegrid')\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "import re\n",
        "import gensim\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "Fvdk4QCUjyTO",
        "outputId": "e360eada-409e-4b56-dda7-4da065a0c41d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'wordcloud'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8d9cd6671dd4>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'whitegrid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation"
      ],
      "metadata": {
        "id": "pXDTdRlBDqCb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygltrAGwW7J6"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the uploaded zip file\n",
        "zip_path = '/content/kagglemoviereviews.zip'\n",
        "extract_folder = '/content/kaggle_movie_reviews'\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_folder)\n",
        "\n",
        "# List the files extracted\n",
        "extracted_files = os.listdir(extract_folder)\n",
        "extracted_files\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List the files in the 'kagglemoviereviews' directory\n",
        "kaggle_reviews_path = os.path.join(extract_folder, 'kagglemoviereviews')\n",
        "review_files = os.listdir(kaggle_reviews_path)\n",
        "review_files\n"
      ],
      "metadata": {
        "id": "qeSScqPNc0qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List the files in the 'corpus' directory\n",
        "corpus_path = os.path.join(kaggle_reviews_path, 'corpus')\n",
        "corpus_files = os.listdir(corpus_path)\n",
        "corpus_files"
      ],
      "metadata": {
        "id": "X22Eq_71c2j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the train.tsv file\n",
        "train_data_path = os.path.join(corpus_path, 'train.tsv')\n",
        "train_df = pd.read_csv(train_data_path, sep='\\t')  # Assuming it's tab-separated based on the .tsv extension\n",
        "\n",
        "train_df.head()\n"
      ],
      "metadata": {
        "id": "sorafP6wc4td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train.tsv file has been successfully loaded into a DataFrame. Here's a look at its structure:\n",
        "\n",
        "PhraseId: An identifier for each phrase.\n",
        "SentenceId: An identifier for each sentence.\n",
        "Phrase: The text of the phrase or sentence.\n",
        "Sentiment: The sentiment classification for each phrase."
      ],
      "metadata": {
        "id": "IF5G1gBLdHqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the train.tsv file\n",
        "test_data_path = os.path.join(corpus_path, 'test.tsv')\n",
        "test_df = pd.read_csv(test_data_path, sep='\\t')  # Assuming it's tab-separated based on the .tsv extension\n",
        "\n",
        "test_df.head()\n"
      ],
      "metadata": {
        "id": "QsQvZyhSc8kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test.tsv file has been successfully loaded into a DataFrame. Here's a look at its structure:\n",
        "\n",
        "PhraseId: An identifier for each phrase.\n",
        "\n",
        "SentenceId: An identifier for each sentence.\n",
        "\n",
        "Phrase: The text of the phrase or sentence."
      ],
      "metadata": {
        "id": "7H-2hmqBdaFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "3bxiq3Tweoea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "GGa-5JSjer1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info()"
      ],
      "metadata": {
        "id": "CgacwBK6etlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.info()"
      ],
      "metadata": {
        "id": "51svVE__e1RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.isnull().sum()"
      ],
      "metadata": {
        "id": "gdXWOgPQe4HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.Sentiment.value_counts()"
      ],
      "metadata": {
        "id": "u58yYTEGfCWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.isnull().sum()"
      ],
      "metadata": {
        "id": "tEzgADOXe8_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the histograms displaying the distribution of word counts for each sentiment category in the training data:\n",
        "\n",
        "Very Negative: Histogram shows the distribution of word counts for reviews labeled as very negative, with an average word count displayed.\n",
        "\n",
        "Negative: Similar plot for negative sentiment.\n",
        "\n",
        "Neutral: Shows the neutral reviews and their word counts.\n",
        "\n",
        "Positive: Visualization for positive reviews.\n",
        "\n",
        "Very Positive: Depicts the word count distribution for very positive reviews."
      ],
      "metadata": {
        "id": "Y-8BFoUkhN6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "PRONi7PyD2BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Categorizing the ratings from 0 to 5 as Negative, Somewhat Negative, Neutral, Somewhat positive, Positive, Failed\n",
        "Sentiment_phrase =[]\n",
        "\n",
        "for row in train_df['Sentiment']:\n",
        "    if row == 0:\n",
        "        Sentiment_phrase.append('negative')\n",
        "    elif row == 1:\n",
        "        Sentiment_phrase.append('somewhat negative')\n",
        "    elif row == 2:\n",
        "        Sentiment_phrase.append('neutral')\n",
        "    elif row == 3:\n",
        "        Sentiment_phrase.append('somewhat positive')\n",
        "    elif row == 4:\n",
        "        Sentiment_phrase.append('positive')\n",
        "    else:\n",
        "        Sentiment_phrase.append('Failed')\n",
        "\n",
        "\n",
        "train_df['Sentiment_phrase'] = Sentiment_phrase"
      ],
      "metadata": {
        "id": "1tMTXun-ihNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Depicting the type of sentiments\n",
        "\n",
        "\n",
        "# Calculate the word count for each phrase in the training data\n",
        "train_df['WordCount'] = train_df['Phrase'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Define sentiment categories for better labeling\n",
        "sentiment_labels = {0: 'Very Negative', 1: 'Negative', 2: 'Neutral', 3: 'Positive', 4: 'Very Positive'}\n",
        "\n",
        "# Create histograms for each sentiment category\n",
        "fig, axes = plt.subplots(3, 2, figsize=(12, 18), sharex=True, sharey=True)\n",
        "axes = axes.flatten()\n",
        "\n",
        "for sentiment, ax in zip(sorted(sentiment_labels.keys()), axes):\n",
        "    data = train_df[train_df['Sentiment'] == sentiment]['WordCount']\n",
        "    ax.hist(data, bins=30, color='skyblue', alpha=0.7)\n",
        "    ax.set_title(f'{sentiment_labels[sentiment]} (Sentiment {sentiment}) - Mean Words: {data.mean():.2f}')\n",
        "    ax.set_xlabel('Word Count')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.grid(True)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PgEq35KyfAwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 histogram shows that the distribution behaves like a negative exponential function, decreasing significantly as the x-axis increases. The longest sentence in the expression column corresponds to the 'Negative Comments' class and seems to consist of around 52 words, now using the max() function the longest sentence can be found"
      ],
      "metadata": {
        "id": "BvKGjliFZR4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#maximum number of words found in a phrase\n",
        "train_df['Phrase'].str.split().map(lambda x: len(x)).max()"
      ],
      "metadata": {
        "id": "YzqlKyaLZUlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Maximum number of words found in a particular sentence\n",
        "train_df['Sentiment_phrase'].value_counts()"
      ],
      "metadata": {
        "id": "OiQBnlvojhQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#count of sentiment phrases\n",
        "train_df['Sentiment_phrase'].value_counts().sort_index().plot(kind='bar', color= 'green')\n",
        "plt.ylabel('Sentiment Phrase Count')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.title('Count of Sentiments');"
      ],
      "metadata": {
        "id": "sNVBQqxtjk-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--------------------------------------------------------------------\")\n",
        "dfff=pd.DataFrame(train_df['Phrase'].str.split().map(lambda x: len(x))>=20)\n",
        "print('Number of sentences which contain more than 20 words: ', dfff.loc[dfff['Phrase']==True].shape[0])\n",
        "print(' ')\n",
        "print(\"--------------------------------------------------------------------\")\n",
        "dfff=pd.DataFrame(train_df['Phrase'].str.split().map(lambda x: len(x))>=30)\n",
        "print('Number of sentences which contain more than 30 words: ', dfff.loc[dfff['Phrase']==True].shape[0])\n",
        "print(' ')\n",
        "print(\"--------------------------------------------------------------------\")\n",
        "dfff=pd.DataFrame(train_df['Phrase'].str.split().map(lambda x: len(x))>=40)\n",
        "print('Number of sentences which contain more than 40 words: ', dfff.loc[dfff['Phrase']==True].shape[0])\n",
        "print(' ')\n",
        "print(\"--------------------------------------------------------------------\")\n",
        "dfff=pd.DataFrame(train_df['Phrase'].str.split().map(lambda x: len(x))>=50)\n",
        "print('Number of sentences which contain more than 50 words: ', dfff.loc[dfff['Phrase']==True].shape[0])\n",
        "print(' ')\n",
        "print(\"--------------------------------------------------------------------\")\n",
        "dfff=pd.DataFrame(train_df['Phrase'].str.split().map(lambda x: len(x))==52)\n",
        "print('Number of sentences which contain 52 words: ', dfff.loc[dfff['Phrase']==True].shape[0])\n",
        "print(' ')\n",
        "print(\"--------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "UY_lbeAyX_io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns"
      ],
      "metadata": {
        "id": "lufgUlFvlcOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of phrases per sentence\n",
        "phrases_per_sentence = train_df.groupby('SentenceId')['PhraseId'].count()\n",
        "\n",
        "# Plotting the distribution of phrases per sentence\n",
        "plt.figure(figsize=(8, 5))\n",
        "phrases_per_sentence.hist(bins=30, color='lightcoral', edgecolor='black')\n",
        "plt.title('Distribution of Phrases per Sentence')\n",
        "plt.xlabel('Number of Phrases')\n",
        "plt.ylabel('Frequency of Sentences')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nYL5uTQUBCWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Depiciting the number of bi-grams, and tri-grams\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def get_top_ngrams(corpus, n=None, ngram_range=(2,2)):\n",
        "    vec = CountVectorizer(ngram_range=ngram_range, stop_words='english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "# Get the top 10 bi-grams and tri-grams\n",
        "top_bi_grams = get_top_ngrams(train_df['Phrase'], n=10, ngram_range=(2,2))\n",
        "top_tri_grams = get_top_ngrams(train_df['Phrase'], n=10, ngram_range=(3,3))\n",
        "\n",
        "top_bi_grams, top_tri_grams\n"
      ],
      "metadata": {
        "id": "Zl8pNkuGY6nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract phrases for negative, neutral, and positive sentiments\n",
        "negative_phrases = train_df[train_df['Sentiment'] == 1]['Phrase'].sample(5, random_state=1).tolist()\n",
        "neutral_phrases = train_df[train_df['Sentiment'] == 2]['Phrase'].sample(5, random_state=1).tolist()\n",
        "positive_phrases = train_df[train_df['Sentiment'] == 3]['Phrase'].sample(5, random_state=1).tolist()\n",
        "\n",
        "negative_phrases, neutral_phrases, positive_phrases\n"
      ],
      "metadata": {
        "id": "Gs-nZWk_6zBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Representation of a wordcloud\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(train_df['Phrase']))\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Phrases')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FSl-U2bM7Pi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Preprocessing"
      ],
      "metadata": {
        "id": "zLZ-pOXm9IVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_df['Phrase'].values\n",
        "y = train_df['Sentiment'].values"
      ],
      "metadata": {
        "id": "cJzLIMyTZC7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3 , stratify = y, random_state = 42)"
      ],
      "metadata": {
        "id": "zzNDFS2wZvXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split Train and Test Data"
      ],
      "metadata": {
        "id": "zvU_NZGx9LvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X, y = train_df.drop(columns=['Sentiment']), train_df['Sentiment']\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "xvb9AGmb97vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.2, stratify=y)"
      ],
      "metadata": {
        "id": "2qEf6Q3p-HT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train Bag of Words Model"
      ],
      "metadata": {
        "id": "LBx6GVk0-Pa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "\n",
        "def tokenizer(text):\n",
        "    # Use regular expressions to match only word characters\n",
        "    # This will split the text into words and remove punctuation\n",
        "    return re.findall(r'\\w+', text.lower())  # Convert to lowercase and find all word characters\n",
        "\n",
        "cv = CountVectorizer(tokenizer=tokenizer)\n",
        "# Presuming X_train['Phrase'] is a column of text data from your DataFrame\n",
        "token_counts = cv.fit_transform(X_train['Phrase'])"
      ],
      "metadata": {
        "id": "p0NDFJru-Og8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_counts.shape"
      ],
      "metadata": {
        "id": "SM6fYF3p-tkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.DataFrame.sparse.from_spmatrix(token_counts, columns=cv.get_feature_names_out())"
      ],
      "metadata": {
        "id": "FY1FBNp7-yBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modeling"
      ],
      "metadata": {
        "id": "2QOJ3Nh--11i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Naive Bayes Model"
      ],
      "metadata": {
        "id": "U7f08qyu-6sJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nbc = MultinomialNB()\n",
        "#Fitting the classifier to the training data\n",
        "nbc.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "QWfT_PjN-9yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the performance of the trained Multinomial Naive Bayes classifier on the training data\n",
        "nbc.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "KpMej5X3_AcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transforms the phrases from the validatoin dataset into a feature matrix\n",
        "val_counts = cv.transform(X_validate['Phrase'])"
      ],
      "metadata": {
        "id": "ZCaBmJQb_ClT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting it into a pandas dataframe then uses the dataframe as inout to make predictions with the trained Naive Bayes classifier\n",
        "X_validate = pd.DataFrame.sparse.from_spmatrix(val_counts, columns=cv.get_feature_names_out())\n",
        "y_pred = nbc.predict(X_validate)"
      ],
      "metadata": {
        "id": "rH9DVjjJ_EVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating the accuracy predictions, precision for predictions, recall for predictions and F1 score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "def print_performance(clf, X, y):\n",
        "    y_pred = clf.predict(X)\n",
        "\n",
        "    accuracy = accuracy_score(y_pred, y)\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    print(\"Precision:\", precision_score(y_pred, y, average='weighted'))\n",
        "    print(\"Recall:   \", recall_score(y_pred, y, average='weighted'))\n",
        "    print(\"F1 Score: \", f1_score(y_pred, y, average='weighted'))\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred) #creating the confusion matrix\n",
        "\n",
        "    #confusion matrix heatmap\n",
        "    plt.figure(figsize=(9,9))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
        "    plt.ylabel('Actual label');\n",
        "    plt.xlabel('Predicted label');\n",
        "\n",
        "    #Displaying the acuuracy score on the heatmap\n",
        "    all_sample_title = 'Accuracy Score: {0}'.format(accuracy)\n",
        "    plt.title(all_sample_title, size = 15);"
      ],
      "metadata": {
        "id": "RsvQqJId_IDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_performance(nbc, X_validate, y_validate)"
      ],
      "metadata": {
        "id": "Z_QXnL8L_JD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decision Tree Classifier"
      ],
      "metadata": {
        "id": "jdThzsyk_NH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting the target values for the validation set and displaying the accuracy, precision, recall, F1 score and plot a confusion matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dtf = DecisionTreeClassifier()\n",
        "dtf.fit(X_train, y_train)\n",
        "\n",
        "print_performance(dtf, X_validate, y_validate)"
      ],
      "metadata": {
        "id": "IFVh3cVQ_L9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TPU code"
      ],
      "metadata": {
        "id": "w6dh2LFUuXx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transfomers"
      ],
      "metadata": {
        "id": "6IsoqTr8vCE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf # importing the TensorFlow library\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver() #attempts to detect the TPU and establish a connection automatically.\n",
        "tf.config.experimental_connect_to_cluster(tpu)# TensorFlow graph to the detected TPU cluster\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)# Initializes the TPU system, configuring it for use by TensorFlow\n",
        "strategy = tf.distribute.TPUStrategy(tpu) #Creating a TPUStrategy object"
      ],
      "metadata": {
        "id": "t9rWD3kruXVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tensorflow libraries\n",
        "from tensorflow.keras.layers import Input, Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.initializers import TruncatedNormal\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "PpNnWELnx7FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = train_df[['Phrase', 'Sentiment']]\n",
        "\n",
        "# Set your model output as categorical and save in new label col\n",
        "data['Sentiment_label'] = pd.Categorical(data['Sentiment'])\n",
        "\n",
        "# Transform your output to numeric\n",
        "data['Sentiment'] = data['Sentiment_label'].cat.codes"
      ],
      "metadata": {
        "id": "nild2rpGyEBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train, data_val = train_test_split(data, test_size=0.1, stratify=data['Sentiment_label'], random_state=42)\n",
        "\n",
        "print(\"Training Data Sentiment_label Distribution:\")\n",
        "print(data_train['Sentiment_label'].value_counts(normalize = True))\n",
        "\n",
        "print(\"\\nValidation Data Sentiment_label Distribution:\")\n",
        "print(data_val['Sentiment_label'].value_counts(normalize = True))"
      ],
      "metadata": {
        "id": "IUAC0SI2yOzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###BERT (Bidirectional Encoder Representations from Transformers)"
      ],
      "metadata": {
        "id": "Lveca9S1yRgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"tensorflow\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"SetPriority unimplemented for this stream.\", category=Warning)\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import logging\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "npVG5u0IyQbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with TPU\n",
        "from transformers import TFBertModel, BertTokenizer, BertConfig, BertTokenizerFast\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from transformers import TFAutoModel\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Max length of tokens\n",
        "max_length = 45\n",
        "\n",
        "def create_model(max_length):\n",
        "    # Load the MainLayer\n",
        "    bert = TFAutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Build your model input\n",
        "    input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
        "    inputs = {'input_ids': input_ids}\n",
        "\n",
        "    # Load the Transformers BERT model as a layer in a Keras model\n",
        "    bert_model = bert(inputs)[1]\n",
        "    dropout = Dropout(0.1, name='pooled_output')  # Assuming config.hidden_dropout_prob is 0.1\n",
        "    pooled_output = dropout(bert_model, training=False)\n",
        "\n",
        "    # Then build your model output\n",
        "    Sentiments = Dense(units=len(data_train.Sentiment_label.value_counts()),\n",
        "                       kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n",
        "                       name='Sentiment')(pooled_output)\n",
        "    outputs = {'Sentiment': Sentiments}\n",
        "    # And combine it all in a model object\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='BERT_MultiClass')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create model inside the strategy scope\n",
        "with strategy.scope():\n",
        "    model = create_model(max_length)\n",
        "\n",
        "    # Set an optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-05, epsilon=1e-08, clipnorm=1.0)\n",
        "\n",
        "    # Set loss and metrics\n",
        "    loss = {'Sentiment': tf.keras.losses.CategoricalCrossentropy(from_logits=True)}\n",
        "\n",
        "    # Compile the model inside the strategy scope\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    # Print model summary\n",
        "    model.summary()\n"
      ],
      "metadata": {
        "id": "wtpZjNMiyuXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ready output data for the model\n",
        "y_train = tf.keras.utils.to_categorical(data_train['Sentiment'])\n",
        "\n",
        "# Tokenize the input (takes some time)\n",
        "x_train = tokenizer(\n",
        "    text=data_train['Phrase'].to_list(),\n",
        "    add_special_tokens=True,\n",
        "    max_length=max_length,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    return_tensors='tf',\n",
        "    return_token_type_ids=False,\n",
        "    return_attention_mask=True,\n",
        "    verbose=True)\n",
        "\n",
        "y_val = tf.keras.utils.to_categorical(data_val['Sentiment'])\n",
        "x_val = tokenizer(\n",
        "    text=data_val['Phrase'].to_list(),\n",
        "    add_special_tokens=True,\n",
        "    max_length=max_length,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    return_tensors='tf',\n",
        "    return_token_type_ids=False,\n",
        "    return_attention_mask=True,\n",
        "    verbose=True)\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(\n",
        "    x={'input_ids': x_train['input_ids']},\n",
        "    y={'Sentiment': y_train},\n",
        "    validation_data=({'input_ids': x_val['input_ids']}, {'Sentiment': y_val}),\n",
        "    batch_size=32,\n",
        "    epochs=2,\n",
        "    verbose=1)"
      ],
      "metadata": {
        "id": "kGTKNQIHzCrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the model using the validation data\n",
        "model_eval = model.evaluate(\n",
        "    x={'input_ids': x_val['input_ids']},\n",
        "    y={'Sentiment': y_val}\n",
        ")"
      ],
      "metadata": {
        "id": "1LqEUyoLzl9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making predictions using the model on the validation datasetl\n",
        "y_val_predicted = model.predict(\n",
        "    x={'input_ids': x_val['input_ids']},\n",
        ")\n",
        "y_val_predicted['Sentiment']"
      ],
      "metadata": {
        "id": "KVBM6bEl3d-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred_max=[np.argmax(i) for i in y_val_predicted['Sentiment']]"
      ],
      "metadata": {
        "id": "F97CQxGEz6TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The index corresponds to the class with the highest predicted probability\n",
        "y_val_actual_max=[np.argmax(i) for i in y_val]"
      ],
      "metadata": {
        "id": "Nku9yMScz7pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#producing the classification report\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = classification_report(y_val_pred_max, y_val_actual_max)\n",
        "\n",
        "print(report)"
      ],
      "metadata": {
        "id": "2ToXQwniz9Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "validation set:"
      ],
      "metadata": {
        "id": "O8kW94Ea4icw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_val_pred_max, y_val_actual_max), display_labels=np.unique(y_val_actual_max))\n",
        "disp.plot(cmap='Blues')\n",
        "plt.grid(False)"
      ],
      "metadata": {
        "id": "_utW80Kzz_qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = tokenizer(\n",
        "          text=test_df['Phrase'].astype(str).to_list(),\n",
        "          add_special_tokens=True,\n",
        "          max_length=max_length,\n",
        "          truncation=True,\n",
        "          padding=True,\n",
        "          return_tensors='tf',\n",
        "          return_token_type_ids = False,\n",
        "          return_attention_mask = False,\n",
        "#is_split_into_words=True,\n",
        "          verbose = True)"
      ],
      "metadata": {
        "id": "MCTNU_DP0J0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_predicted = model.predict(\n",
        "    x={'input_ids': x_test['input_ids']},\n",
        ")\n"
      ],
      "metadata": {
        "id": "7hImXJAe0Nyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_predicted['Sentiment']"
      ],
      "metadata": {
        "id": "Z0gDtqtw0Orx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_pred_max=[np.argmax(i) for i in label_predicted['Sentiment']]"
      ],
      "metadata": {
        "id": "Llfft-oQ0Ro7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting the outputs from the validation set using model\n",
        "from sklearn.metrics import f1_score, recall_score, average_precision_score\n",
        "\n",
        "y_val_pred_max = model.predict(x_val['input_ids'])['Sentiment']  # 'Sentiment' çıkışını alın\n",
        "\n",
        "y_pred = np.argmax(y_val_pred_max, axis=1)\n",
        "\n",
        "y_pred_proba = tf.nn.softmax(y_val_pred_max)\n",
        "\n",
        "# Calculating average precision score, which summarizes the precision-recall curve as the weighted mean of precisions at each threshold\n",
        "BERT_AP = average_precision_score(y_val, y_pred_proba)\n",
        "# Calculating the weighted F1 score, which considers both precision and recall to balance the contributions of false positives and false negatives\n",
        "BERT_f1 = f1_score(np.argmax(y_val, axis=1), y_pred, average='weighted')\n",
        "# Calculating the weighted recall, which is the ability of the classifier to find all the positive samples, weighted by the prevalence of each class\n",
        "BERT_rec = recall_score(np.argmax(y_val, axis=1), y_pred, average='weighted')\n",
        "# Printing out the calculated metrics\n",
        "print(\"Average Precision:\", BERT_AP)\n",
        "print(\"Weighted F1 Score:\", BERT_f1)\n",
        "print(\"Weighted Recall:\", BERT_rec)"
      ],
      "metadata": {
        "id": "U0K-R4It0VzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a summary report for all the models accuracy rate.\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "x = PrettyTable()\n",
        "\n",
        "x.field_names = ['Model', 'Accuracy']\n",
        "x.add_row(['Naive Bayes', '60.8%'])\n",
        "x.add_row(['Decision Tree', '59.5%'])\n",
        "x.add_row(['BERT', '68%'])\n",
        "print(x)"
      ],
      "metadata": {
        "id": "bLHIXZok88gH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}